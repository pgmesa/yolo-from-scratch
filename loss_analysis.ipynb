{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## YOLOv5 loss analysis, as performed in the medium article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from ultralytics_tools.metrics import bbox_iou\n",
    "from loss import YOLOv5Loss, YOLOv3Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss calculation example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Targets Shape: torch.Size([5, 6])\n",
      "Anchors Shape: torch.Size([3, 3, 2])\n",
      "Layer P3 Shape: torch.Size([2, 3, 40, 40, 25])\n",
      "Layer P4 Shape: torch.Size([2, 3, 20, 20, 25])\n",
      "Layer P5 Shape: torch.Size([2, 3, 10, 10, 25])\n"
     ]
    }
   ],
   "source": [
    "device = 'cpu'\n",
    "img_size = 320\n",
    "num_classes = 20; num_layers = 3\n",
    "anchor_t = 4.0\n",
    "\n",
    "# Loss weights\n",
    "balance = [4.0, 1.0, 0.4]\n",
    "lambda_box = 0.05; lambda_obj = 0.7; lambda_cls = 0.3\n",
    "\n",
    "anchors = torch.tensor([\n",
    "    # Anchor_1 for each prediction layer\n",
    "    [[ 1.25000,  1.62500],[ 2.00000,  3.75000],[ 4.12500,  2.87500]], \n",
    "    # Anchor_2 for each prediction layer\n",
    "    [[ 1.87500,  3.81250],[ 3.87500,  2.81250],[ 3.68750,  7.43750]], \n",
    "    # Anchor_3 for each prediction layer\n",
    "    [[ 3.62500,  2.81250],[ 4.87500,  6.18750],[11.65625, 10.18750]], \n",
    "])\n",
    "num_anchors = anchors.shape[0]\n",
    "\n",
    "targets = torch.tensor([\n",
    "        [ 0.00000, 14.00000,  0.49535,  0.50528,  0.15267,  0.56956],\n",
    "        [ 0.00000,  0.00000,  0.54872,  0.92491,  0.05361,  0.03183],\n",
    "        [ 0.00000,  0.00000,  0.36780,  0.98716,  0.06031,  0.02567],\n",
    "        [ 1.00000,  6.00000,  0.97072,  0.04398,  0.05856,  0.08796],\n",
    "        [ 1.00000, 16.00000,  0.70696,  0.10348,  0.32971,  0.16793],\n",
    "])\n",
    "batch_size = len(targets[:,:1].unique())\n",
    "\n",
    "strides = [8, 16, 32]\n",
    "p = [\n",
    "    torch.randn((batch_size, num_anchors, img_size//strides[i], img_size//strides[i], 5 + num_classes)) \n",
    "    for i in range(num_layers)\n",
    "]\n",
    "\n",
    "print(\"Targets Shape:\", targets.shape)\n",
    "print(\"Anchors Shape:\", anchors.shape)\n",
    "for i, pi in enumerate(p):\n",
    "    print(f\"Layer P{i+3} Shape:\", pi.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Original YOLOv5 loss function with minor adjustments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ComputeLoss:\n",
    "    sort_obj_iou = False\n",
    "\n",
    "    # Compute losses\n",
    "    def __init__(self, autobalance=False, anchor_t:int=4, balance:list[int]=[4.0, 1.0, 0.4], \n",
    "                 strides=[8,16,32], lambda_box=0.05, lambda_obj=0.7, lambda_cls=0.3):\n",
    "        \"\"\"Initializes ComputeLoss with model and autobalance option, autobalances losses if True.\"\"\"\n",
    "        # Define criteria\n",
    "        BCEcls = nn.BCEWithLogitsLoss()\n",
    "        BCEobj = nn.BCEWithLogitsLoss()\n",
    "\n",
    "        # Class label smoothing https://arxiv.org/pdf/1902.04103.pdf eqn 3\n",
    "        self.cp, self.cn = 1.0, 0.0\n",
    "        \n",
    "        self.balance = balance\n",
    "        \n",
    "        self.ssi = strides.index(16) if autobalance else 0  # stride 16 index\n",
    "        self.BCEcls, self.BCEobj, self.gr, self.autobalance = BCEcls, BCEobj, 1.0, autobalance\n",
    "        self.na = num_anchors  # number of anchors\n",
    "        self.nc = num_classes  # number of classes\n",
    "        self.nl = num_layers  # number of layers\n",
    "        self.anchors = anchors\n",
    "        self.device = device\n",
    "        \n",
    "        self.anchor_t = anchor_t\n",
    "        self.lambda_box = lambda_box\n",
    "        self.lambda_obj = lambda_obj\n",
    "        self.lambda_cls = lambda_cls\n",
    "        \n",
    "\n",
    "    def __call__(self, p, targets):  # predictions, targets\n",
    "        \"\"\"Performs forward pass, calculating class, box, and object loss for given predictions and targets.\"\"\"\n",
    "        lcls = torch.zeros(1, device=self.device)  # class loss\n",
    "        lbox = torch.zeros(1, device=self.device)  # box loss\n",
    "        lobj = torch.zeros(1, device=self.device)  # object loss\n",
    "        \n",
    "        tcls, tbox, indices, anchors = self.build_targets(p, targets)  # targets\n",
    "        \n",
    "        # Losses\n",
    "        for i, pi in enumerate(p):  # layer index, layer predictions\n",
    "            b, a, gj, gi = indices[i]  # image, anchor, gridy, gridx\n",
    "            tobj = torch.zeros(pi.shape[:4], dtype=pi.dtype, device=self.device)  # target obj\n",
    "            \n",
    "            n = b.shape[0]  # number of targets\n",
    "            if n:\n",
    "                # pxy, pwh, _, pcls = pi[b, a, gj, gi].tensor_split((2, 4, 5), dim=1)  # faster, requires torch 1.8.0\n",
    "                pxy, pwh, _, pcls = pi[b, a, gj, gi].split((2, 2, 1, self.nc), 1)  # target-subset of predictions\n",
    "\n",
    "                # Regression\n",
    "                pxy = pxy.sigmoid() * 2 - 0.5\n",
    "                pwh = (pwh.sigmoid() * 2) ** 2 * anchors[i]\n",
    "                pbox = torch.cat((pxy, pwh), 1)  # predicted box\n",
    "                iou = bbox_iou(pbox, tbox[i], CIoU=True).squeeze()  # iou(prediction, target)\n",
    "                lbox += (1.0 - iou).mean()  # iou loss\n",
    "\n",
    "                # Objectness\n",
    "                iou = iou.detach().clamp(0).type(tobj.dtype)\n",
    "                if self.sort_obj_iou:\n",
    "                    j = iou.argsort()\n",
    "                    b, a, gj, gi, iou = b[j], a[j], gj[j], gi[j], iou[j]\n",
    "                if self.gr < 1:\n",
    "                    iou = (1.0 - self.gr) + self.gr * iou\n",
    "                tobj[b, a, gj, gi] = iou  # iou ratio\n",
    "\n",
    "                # Classification\n",
    "                if self.nc > 1:  # cls loss (only if multiple classes)\n",
    "                    t = torch.full_like(pcls, self.cn, device=self.device)  # targets\n",
    "                    t[range(n), tcls[i]] = self.cp\n",
    "                    lcls += self.BCEcls(pcls, t)  # BCE\n",
    "\n",
    "                # Append targets to text file\n",
    "                # with open('targets.txt', 'a') as file:\n",
    "                #     [file.write('%11.5g ' * 4 % tuple(x) + '\\n') for x in torch.cat((txy[i], twh[i]), 1)]\n",
    "\n",
    "            obji = self.BCEobj(pi[..., 4], tobj)\n",
    "            lobj += obji * self.balance[i]  # obj loss\n",
    "            if self.autobalance:\n",
    "                self.balance[i] = self.balance[i] * 0.9999 + 0.0001 / obji.detach().item()\n",
    "\n",
    "        if self.autobalance:\n",
    "            self.balance = [x / self.balance[self.ssi] for x in self.balance]\n",
    "        lbox *= self.lambda_box\n",
    "        lobj *= self.lambda_obj\n",
    "        lcls *= self.lambda_cls\n",
    "        bs = tobj.shape[0]  # batch size\n",
    "\n",
    "        return (lbox + lobj + lcls) * bs, torch.cat((lbox, lobj, lcls)).detach()\n",
    "\n",
    "    def build_targets(self, p, targets):\n",
    "        \"\"\"Prepares model targets from input targets (image,class,x,y,w,h) for loss computation, returning class, box,\n",
    "        indices, and anchors.\n",
    "        \"\"\"\n",
    "        na, nt = self.na, targets.shape[0]  # number of anchors, targets\n",
    "        tcls, tbox, indices, anch = [], [], [], []\n",
    "        gain = torch.ones(7, device=self.device)  # normalized to gridspace gain\n",
    "        ai = torch.arange(na, device=self.device).float().view(na, 1).repeat(1, nt)  # same as .repeat_interleave(nt)\n",
    "        targets = torch.cat((targets.repeat(na, 1, 1), ai[..., None]), 2)  # append anchor indices\n",
    "\n",
    "        g = 0.5  # bias\n",
    "        off = (\n",
    "        torch.tensor(\n",
    "            [\n",
    "                [0, 0],\n",
    "                [1, 0],\n",
    "                [0, 1],\n",
    "                [-1, 0],\n",
    "                [0, -1],  # j,k,l,m\n",
    "                # [1, 1], [1, -1], [-1, 1], [-1, -1],  # jk,jm,lk,lm\n",
    "            ],\n",
    "            device=self.device,\n",
    "        ).float()\n",
    "        * g\n",
    "    )  # offset\n",
    "    \n",
    "        for i in range(self.nl):\n",
    "            anchors, shape = self.anchors[i], p[i].shape\n",
    "            gain[2:6] = torch.tensor(shape)[[3, 2, 3, 2]]  # xyxy gain\n",
    "\n",
    "            # Match targets to anchors\n",
    "            t = targets * gain  # shape(3,n,7)\n",
    "\n",
    "            if nt:\n",
    "                # Matches\n",
    "                r = t[..., 4:6] / anchors[:, None]  # wh ratio\n",
    "                j = torch.max(r, 1 / r).max(2)[0] < anchor_t  # compare\n",
    "                # j = wh_iou(anchors, t[:, 4:6]) > model.hyp['iou_t']  # iou(3,n)=wh_iou(anchors(3,2), gwh(n,2))\n",
    "                t = t[j]  # filter\n",
    "\n",
    "                # Offsets\n",
    "                gxy = t[:, 2:4]  # grid xy\n",
    "                gxi = gain[[2, 3]] - gxy  # inverse\n",
    "                j, k = ((gxy % 1 < g) & (gxy > 1)).T\n",
    "                l, m = ((gxi % 1 < g) & (gxi > 1)).T\n",
    "                j = torch.stack((torch.ones_like(j), j, k, l, m))\n",
    "                t = t.repeat((5, 1, 1))[j]\n",
    "                offsets = (torch.zeros_like(gxy)[None] + off[:, None])[j]\n",
    "            else:\n",
    "                t = targets[0]\n",
    "                offsets = 0\n",
    "\n",
    "            # Define\n",
    "            bc, gxy, gwh, a = t.chunk(4, 1)  # (image, class), grid xy, grid wh, anchors\n",
    "            a, (b, c) = a.long().view(-1), bc.long().T  # anchors, image, class\n",
    "            gij = (gxy - offsets).long()\n",
    "            gi, gj = gij.T  # grid indices\n",
    "\n",
    "            # Append\n",
    "            indices.append((b, a, gj.clamp(0, shape[2] - 1), gi.clamp(0, shape[3] - 1)))  # image, anchor, grid\n",
    "            tbox.append(torch.cat((gxy - gij, gwh), 1))  # box\n",
    "            anch.append(anchors[a])  # anchors\n",
    "            tcls.append(c)  # class\n",
    "\n",
    "        return tcls, tbox, indices, anch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare original YOLOv5 loss with modified version (should output the same)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "ultralytics_loss = ComputeLoss(anchor_t=anchor_t, balance=balance, strides=strides, lambda_box=lambda_box, lambda_cls=lambda_cls, lambda_obj=lambda_obj)\n",
    "cleaned_loss = YOLOv5Loss(anchors, anchor_t=anchor_t, balance=balance, lambda_box=lambda_box, lambda_cls=lambda_cls, lambda_obj=lambda_obj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that cleaned YOLOv5 loss version outputs the same results as the original one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============== YOLOv5 ===============\n",
      "Number of built-targets: P3:30 | P4:21 | P5:12\n",
      "[%] All built-targets match\n",
      "======================================\n",
      "Ultralytics YOLOv5 loss:     tensor([7.7861]) tensor([0.1374, 3.0279, 0.7277])\n",
      "Cleaned YOLOv5 loss version: tensor([7.7861]) tensor([0.1374, 3.0279, 0.7277])\n",
      "[%] All losses match\n",
      "======================================\n"
     ]
    }
   ],
   "source": [
    "# Build targets\n",
    "print(\"=============== YOLOv5 ===============\")\n",
    "tcls, tbox, indices, anch = ultralytics_loss.build_targets(p, targets)\n",
    "tcls2, tbox2, indices2, anchors2 = cleaned_loss.build_targets(p, targets)\n",
    "\n",
    "for i, (t1, t2) in enumerate(zip(tcls, tcls2)):\n",
    "    assert len(t1) == len(t2), f\"Number of built-targets doesn't match in layer P{i+3} -> {len(tcls)} - {len(tcls2)}\"\n",
    "print(f\"Number of built-targets: P3:{len(tcls[0])} | P4:{len(tcls[1])} | P5:{len(tcls[2])}\")\n",
    "\n",
    "for i in range(len(p)):\n",
    "    assert torch.equal(tcls[i], tcls2[i]), \"Target classes don't match\"\n",
    "    assert torch.equal(tbox[i], tbox2[i]), \"Target boxes don't match\"\n",
    "    for ind1, ind2 in zip(indices[i], indices2[i]):\n",
    "        assert torch.equal(ind1, ind2), \"Indices don't match\"\n",
    "    assert torch.equal(anch[i], anchors2[i]), \"Anchors don't match\"\n",
    "print(\"[%] All built-targets match\")\n",
    "print(\"======================================\")\n",
    "\n",
    "# Compute loss\n",
    "total_loss, loss_items = ultralytics_loss(p, targets)\n",
    "total_loss2, loss_items2 = cleaned_loss(p, targets)\n",
    "\n",
    "print(\"Ultralytics YOLOv5 loss:    \", total_loss, loss_items)\n",
    "print(\"Cleaned YOLOv5 loss version:\", total_loss2, loss_items2)\n",
    "\n",
    "assert torch.equal(total_loss, total_loss2), \"Total loss doesn't match\"\n",
    "assert torch.equal(loss_items, loss_items2), \"Loss items don't match\"\n",
    "print(\"[%] All losses match\")\n",
    "print(\"======================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test YOLOv3 loss implementation and compare results with YOLOv5\n",
    "\n",
    "> YOLOv3 creates much less built-targets than YOLOv5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============== YOLOv3 ===============\n",
      "Number of built-targets: P3:3 | P4:3 | P5:2\n",
      "Custom YOLOv3 loss: tensor([8.2952]) tensor([0.2516, 2.0245, 1.1832, 0.6883])\n",
      "======================================\n"
     ]
    }
   ],
   "source": [
    "yolov3_loss = YOLOv3Loss(anchors,  iou_t=0.2, lambda_box=lambda_box, lambda_cls=lambda_cls, lambda_obj=lambda_obj)\n",
    "\n",
    "print(\"=============== YOLOv3 ===============\")\n",
    "tcls3, *_ = yolov3_loss.build_targets(p, targets)\n",
    "print(f\"Number of built-targets: P3:{len(tcls3[0])} | P4:{len(tcls3[1])} | P5:{len(tcls3[2])}\")\n",
    "\n",
    "total_loss3, loss_items3 = yolov3_loss(p, targets)\n",
    "print(\"Custom YOLOv3 loss:\", total_loss3, loss_items3)\n",
    "print(\"======================================\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base-3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
